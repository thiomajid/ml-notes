## 1. Transformers models


De fa√ßon g√©n√©rale, la biblioth√®que #transformers[^1] fournit la fonction #pipeline[^2]. C'est l'API de plus haut niveau fournit par la biblioth√®que et est facile √† utiliser.
De fa√ßon g√©n√©rale ce qui se passe dans la fonction `pipeline` est ce qui suit:

```mermaid
graph LR

input[(Data)] -- Raw text--> tokenizer(Tokenizer) -- Input IDs --> model(Model) --Logits --> post_processing(Post-processing)

style input fill:#9FB1BC,stroke:#637777,stroke-width:2px,color:black; 
style tokenizer fill:#4CAF50,stroke:#2E8B57,stroke-width:2px,color:white;
style model fill:#FF9800,stroke:#FF5722,stroke-width:2px,color:white;
style post_processing fill:#03A9F4,stroke:#0277BD,stroke-width:2px,color:white;
```

```py
from transformers import pipeline

classfier = pipeline("sentiment-analysis")
predictions = classfier("We are very happy to show you the ü§ó Transformers library.")
```

Et les pr√©dictions retourner sont de la forme:
```py
[
¬† ¬† {'label': 'POSITIVE', 'score': 0.9997795224189758}
]
```

>[!Warning]
>Le label des pr√©dictions peut √™tre soit *POSITIVE*, soit *NEGATIVE*.

La biblioth√®que transformers fournit √©galement des tokenizers, on peut en utiliser comme suit:
```py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
```

Le tokenizer retourne un r√©sultat de type:
```py
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

On peut √©galement obtenir un mod√®le √† partir d'un #checkpoint en utilisant `AutoModel.from_pretrained(checkpoint)`. Ainsi, suivant la t√¢che √† r√©aliser il existe diff√©rent classes de types *Auto(something)*. On peut citer par exemple:
- `*Model`¬†(retrieve the hidden states)
- `*ForCausalLM`
- `*ForMaskedLM`
- `*ForMultipleChoice`
- `*ForQuestionAnswering`
- `*ForSequenceClassification`
- `*ForTokenClassification`
- and others ü§ó

>[!Warning]
>Les mod√®les de transformers retournes toujours des #logits donc il nous faut appliquer une derni√®re transformation afin d'obtenir des probabilit√©s.
>Egalement on peut consulter le mappage de classes du mod√®le via `model.config.id2label`.


## 2. Using ü§ó transformers
### 2.1 - Cr√©er un transformer
A chaque mod√®le est associ√© une classe de configuration √† partir de laquelle le mod√®le est instanci√©. Instancier un mod√®le directement avec la configuration fait qu'il soit al√©atoirement initialis√© et un tel mod√®le n'est pas bon car il faudra l'entrainer √† nouveau, d'o√π l'int√©r√™t de la m√©thode `from_pretrained`.

En utilisant la m√©thode *from_pretrained*, on peut √©galement modifier certains param√®tres par d√©faut de la configuration du mod√®le pour l'adapter √† nos besoins √©galement.

Apr√®s avoir entrainer le transformer, on peut le sauvegarder, lui et sa configuration, en utilisant la m√©thode `save_pretrained`. Cette m√©thode cr√©er deux fichiers au chemin indiquer:
- **config.json**
- **pytorch_model.bin**

### 2.2 - Tokenizers
#HuggingFace fournit 3 types de tokenizers:
- **Word based**
- **Character based**
- **Sub-word based**

#### Word based tokenizers
Ces tokenizers se basent sur les espaces, les ponctuations ou d'autres r√®gles pour diviser la s√©quences en diff√©rents mots.
Cependant, il y a certaines limites; par exemple *dog* et *dogs* bien qu'√©tant similaires seront consid√©r√©s comme des tokens diff√©rents. 
Egalement on peut se retrouver avec un √©norme #vocabulaire ce qui conduit √† la cr√©ation d'un lourd mod√®le. Pour pallier √† cela, on peut indiquer au tokenizer des mots √† *ignorer*, ou bien lui dire de prendre les *n mots les plus fr√©quents*. 

#### Character based tokenizers
Les vocabulaires cr√©√©s avec ce genre de tokenizers sont de tailles raisonnables car dans les langues, le nombre de lettres est relativement petit contrairement au nombre de mots d'une langue.
Cependant, ce genre de tokenizer fait qu'on perde le sens d'une phrase car les lettres ne fournissent pas d'information sur le contexte de la phrase.
Egalement avec ce genre de tokenizers, on se retrouve avec de trop grandes s√©quences que notre mod√®le doit traiter, ce qui n'est pas id√©ale contrairement aux s√©quences de mots.

#### Sub-word based tokenizers
Pour b√©n√©ficier des atouts des pr√©c√©dents types de tokenizers, on a recours au sub-word based tokenizers.


## 3. Tokenizers library

Il est possible d'entrainer un tokenizer √† partir d'un existant lorsqu'on veut entrainer un mod√®le sur un langage diff√©rent du langage initial. Du genre entrainer *bert-base-uncased* sur un texte *chinois*. Pour ce faire, il suffit d'utiliser la m√©thode:

```py
AutoTokenizer.train_new_from_iterator(**kwargs)
```
























[^1]: [ü§ó Transformers (huggingface.co)](https://huggingface.co/docs/transformers/index)

[^2]: [Quick tour (huggingface.co)](https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines#transformers.pipeline)